<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Inference &amp; Meaning 2004 ~ Papers</title>
<link rel="alternate" type="application/rss+xml" title="RSS" href="https://consequently.org/im2004/index.xml" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css" media="all">@import "white.css";</style>
</head>
<body>
<div class="gray">
<h1>Inference &amp; Meaning 2004 ~ Papers</h1>
</div>
<div id="firstright">
<p>This page contains a list of papers for the <em>Inference and Meaning</em> 2004 conference at the University of Melbourne, to be held from July 12 to 14.</p>
</div>

<div id="main">

<h2>Invited Addresses</h2>
<div class="gray"><h3>Karen Green (Monash)</h3></div>
<div class="title"><p>A Dummettian response to Armstrong and Martin</p>
</div>
<div class="abstract"><p>To infer <em>A</em> from <em>B</em> is, roughly, to be disposed, in any circumstance which inclines one to accept the truth of <em>A</em>, to be similarly inclined to accept the truth of <em>B</em>. Thus characterised, inference is a kind of disposition. Enriched by a behavioural characterisation of the acceptance of truth, an account of inference along these lines might make up part of a behaviourist program of explaining mental acts in behavioural terms. Similarly, at least part of what it is to understand the meaning of a sentence <em>S</em> is that one who understands the sentence will be disposed to accept the truth of <em>S</em> in circumstances which warrant its assertion. An account of the understanding of meaning along these lines might also make up part of a behaviourist program of explaining mental states in behavioural terms. Within the philosophy of mind, however, behaviourism has become a relatively unpopular doctrine, which many (at least in Australia) take to have been superseded by central state materialism. In this paper I examine David Armstrong&#8217;s original argument for replacing a behaviourist reduction of the mental with a central state materialist one. Developing an argument of Michael Dummett&#8217;s, I offer, in the earlier parts of the paper, a Dummettian response to Armstrong’s argument. In the later parts I use the insights of the earlier discussion to respond directly to an argument of Charles Martin&#8217;s. Martin&#8217;s argument attempts to show that the style of reasoning developed by Armstrong (which was in fact originally inspired by Martin) can be used to show that a Dummettian characterisation of the understanding of meaning must collapse. I argue by contrast that there is a simple reply to be made to Martin&#8217;s argument and that the reply enhances the plausibility of Dummett’s position.</p>
</div>
<div class="gray"><h3>Mark Lance (Georgetown)</h3></div>
<div class="title"><p>&#8216;Yo&#8217; and &#8216;Lo&#8217;: the pragmatics of the space of reasons</p>
</div>
<div class="abstract"><p>[This paper is based on joint work with Rebecca Kukla.]</p>

<p>Inferentialists have had surprisingly little systematic to say about what inference and inferential propriety is.  It is widely seen that an inferentialist cannot also claim that inferential propriety is constituted by truth-preservation, but this is merely a negative observation.  To sweep grandly without constraint of argument: all attempts to go beyond this negative point are either incorrect, or seriously incomplete.  (This paper does not fill in the negative argument, but rather assumes its conclusion as a motivation for developing a novel account.)</p>

<p>We begin to address the question of the nature of and relation between inference, inferential propriety, and lingusitic practice, by way of a discussion of pragmatics.  Utilizing two orthogonal abstract distinctions regarding the normative structure of speech acts, we produce a conceptually rich two-by-two matrix of categorization.  The conceptual framework induced by this matrix sheds light on such diverse issues as the metaphysics of normativity and the nature of observation.  It also allows us to see far more clearly the genus of normative transition of which inference is a species.</p>
</div>
<div class="gray"><h3>Jaroslav Peregrin (Prague)</h3></div>
<div class="title"><p>Can we see inference as the ultimate source of meaning?</p>
</div>
<div class="abstract"><p>Contemporary theories of meaning can be roughly divided into those seeing the meaning of an expression as principally a matter of what the expression denotes or stands for, and those seeing it as a matter of how the expression is used. A prominent place among the latter is assumed by those which identify the semantically relevant aspect of the usage of an expression with an inferential pattern governing it. According to these theories, the meaning of an expression is, principally, its <em>inferential role</em>.</p>

<p>However, not all of the meanings which seem to be associated with languages we use are easily seen as creatures of inferences &#8212; this is already the case of meanings of some of the classical logical connectives, not to speak about those of standard second-order quantifiers. Can we maintain the thesis that meaning is <em>generally</em> a matter of inference in view of such facts? Or are we going to claim that these are in some sense non-meanings? </p>

<p>I think that it is important to first establish a framework in which this kind of questions can be articulated with some rigour. My proposal is to see semantics in general as a matter of a delimitation of the space of acceptable truth-valuations for a language, and thus to turn the question about the inferential grounding of meaning into the question which of such spaces are delimitable in terms of <em>inferences</em>. We must also decide what exactly we will take as an inference: a single-consequent rule of the natural deduction kind, or a multiple-consequent Gentzen sequent? (If the latter, then the meanings of classical operators no longer stay outside of the scope of our inferentialism, but the options seems to be less natural.) And, last but not least, we must also decide what kinds lists of inferences&#8217; we will allow to delimit this space: finite, finitely-generated, or even infinite? This creates room for a relatively wide variety of kinds of inferentialism.</p>

<p>I vote for the option connected with natural deduction. If we accept this, then we should also see intuitionist logic as the most natural logic. However this does not preclude the way to classical logic, which is surely natural in some other respects and whose utter inaccessibility would be, I believe, a failure of inferentialism. (Note that inferentialism is a <em>descriptive</em> project concerned with the question <em>what is meaning?</em>; whereas the natural deduction program is more a prescriptive program concerned with the question <em>How should we do logic?</em>. Thus while the latter could perhaps simply ban classical logic if it concluded that one can make do without it, the former is bound to take the extant meanings at face value and face the question <em>If meaning is an inferential matter, then how could there be meanings that are</em> prima facie <em>&#8216;non-inferential&#8217;?</em>) Hence I think that inferentialism, though it may be &#8216;favoring&#8217; some meanings over others, does not result into any kind of unnatural &#8216;semantic ascetism&#8217;. I am convinced that the thesis that all meanings are, ultimately, creatures of inferences &#8212; that they are ultimately inferential roles &#8212; is viable.</p>
</div>
<div class="gray"><h3>Huw Price (Sydney)</h3></div>
<div class="title"><p>&#8220;Not&#8221; again</p>
</div>
<div class="abstract"><p>This talk revisits some views about negation I defended in two early papers, in 1983 and 1990. Some of the themes of those papers have been developed sympathetically in recent work by Tim Smiley, Lloyd Humberstone and Ian Rumfitt. However, Rumfitt and Peter Gibbard have both criticised arguments I offered in defence of Double Negation Elimination (DNE), against a Dummettian intuitionist. I reconsider those arguments, suggesting that although they are better than Rumfitt and Gibbard think, the case against Dummett is for other reasons less straightforward than I took it to be. With reference to Rumfitt&#8217;s own defence of DNE, I call attention to the attractions of the view defended in my 1990 paper, that negation has dialectical origins. But I also point out some difficulties for this view, that seem to have been ignored by previous writers (including me).</p>
</div>
<div class="gray"><h3>G&ouml;ran Sundholm (Leiden)</h3></div>
<div class="title"><p>Constructive Truth-Making</p>
</div>
<div class="abstract"><p>An attempt will be made to set out the constructive (&#8220;intuitionistic&#8221;) meaning-explanations in such a fashion that they constitute a truth-maker analysis of truth (for propositions). Particular care will be taken to set out the theory in an as neutral way as possible, and to identify the place where it turns constructive. Five different notions of truth will then be considered, namely, (1) Tarski&#8217;s propositional function </p>

<blockquote>
<pre><code>*Tr*&lt;sub&gt;L&lt;/sub&gt;(*x*): prop, when *x*: wff&lt;sub&gt;L&lt;/sub&gt;
</code></pre>
</blockquote>

<p>(2) <em>propositional truth</em>, (3) <em>truth for</em> (judgemental and other) <em>contents</em>, (4) <em>correctness</em> (truth) for assertions (and judgements), and (5) <em>rightness</em> (TRUTH).</p>

<p><strong>Background Reading</strong></p>

<ol>
<li><p>&#8220;Existence, Proof and Truth-Making: A Perspective on the Intuitionistic Conception of Truth,&#8221; <em>Topoi</em> Vol. 13 (1994), pp. 117&#8212;126;</p></li>
<li><p>&#8220;Implicit Epistemic Aspects of Constructive Logic&#8221;, <em>Journal of Logic, Language and Information</em>, Vol. 6 (1997), pp. 191&#8212;212;</p></li>
<li><p>&#8220;The Proof-Explanation of Logical Constants is Logically Neutral&#8221;, forthcoming in <em>Rev. Int. de Philosophie</em> (special issue on intuitionism).</p></li>
</ol>
</div>


<h2>Submitted Papers</h2>
<div class="gray"><h3>M. J. Cresswell (Texas A&amp;M and Auckland)</h3></div>
<div class="title"><p>Does Semantics need Infinitary Logic?</p>
</div>
<div class="abstract"><p>The possible-worlds semantics for modality says that a sentence of the form
&#8216;possibly <em>A</em>&#8217; is true in a world <em>w</em> iff <em>A</em> itself is true in some <em>w&#8217;</em>
accessible from <em>w&#8217;</em>. Given any set of propositions, given a notion of the
consistency or satisfiability of a set, and given classical propositional
logic, one can easily prove that every consistent set of propositions can
be embedded in a &#8216;maximal consistent set&#8217; in which for every <em>p</em> either <em>p</em> is
in the set or not-<em>p</em> is in the set. Such sets in a sense represent possible
worlds. However the construction depends on the fact that standard modal
logics are finitary, and it seems false that an infinite collection of sets
of sentences each finite subset of which is intuitively &#8216;possible&#8217; in
natural language has the property that the whole set is possible. Take the
set &#8216;Fred has not counted infinitely many numbers&#8217;, together with the set
&#8216;Fred has counted up to <em>n</em>&#8217;, for each <em>n</em>. Every finite subset of this may be
intuitively possible if Fred is a prodigy, but the whole set may not be. In
first-order logic the problem can be fixed providing &#8216;witnesses&#8217; for
quantifiers, but the desideratum is a solution which makes no reference to the syntactic form of a sentence, except for the ability to form its negation. The
argument of the paper is that the principles needed to shew that natural
language possibility sentnecs involve quantification over worlds are
analogous to those used in infinitary modal logic. The paper is not itself
a contribution to infinitary logic, but a demonstration that work in
semantics may be logically more complex than logicians have believed.</p>
</div>
<div class="gray"><h3>John Fox (La Trobe)</h3></div>
<div class="title"><p>On What Theories of Meaning Could Meanings be Constituted and Explained by Inference Rules?</p>
</div>
<div class="abstract"><p>Gentzen appealingly suggested that the meanings of logical constants could be taken as constituted by their having the intelim rules they do.</p>

<p>But arguments based on Prior (&#8220;The Runabout Inference Ticket&#8221;) and Quine (&#8220;Truth by Convention&#8221;) seem to show that it is not defensible to take the meanings of logical constants or basic logical constants as constituted and given by their inference rules.</p>

<p>However, it is hard to escape the &#8220;theory-ladennness&#8221; of observation language. This doctrine partly suggests and partly rests on the idea that meanings generally are at least largely constituted and given by the inferences they are taken to license.  If this is so, the still entrenched ideas that truths in virtue of meaning are either a priori or conventional must go.</p>

<p>If these are jettisoned, the Prior/Quine arguments are not so worrying for inferential accounts of the meaning even of logical notions. </p>
</div>
<div class="gray"><h3>Mario G&oacute;mez-Torrente (Barcelona and UNAM Mexico)</h3></div>
<div class="title"><p>What Was Tarski&#8217;s Thesis about Logical Truth? And Is It True?  </p>
</div>
<div class="abstract"><p>Let me refer with <em>Tarski&#8217;s Thesis</em> to the strongest thesis that Tarski postulated asserting the  extensional equivalence between a certain pretheoretical concept of logical truth and his technical  concept of logical truth. In this talk I make explicit a few theses about logical truth that sound  Tarskian somehow, including one that most deserves the name <em>Tarski&#8217;s Thesis</em>. And for each of  these theses I claim that either it is presumably true but too weak, or it is too strong and false, or it is  appropriately strong but also false.  </p>

<p>One thesis that in my view is probably true, but is not the strongest thesis Tarski postulated, is   </p>

<blockquote>
(T1) A sentence of a classical propositional/quantificational language is logically true  in the relevant pretheoretical sense iff it is true in all classical  propositional/quantificational models which (re)interpret its constants (other than  its classical propositional/quantificational logical constants).
</blockquote>    

<p>(T1) talks about a very restricted set of logical constants, while Tarski clearly did not just have in  mind this particular set.  A second thesis is    </p>

<blockquote>
(T2) A sentence of a formal language which possibly extends a classical  propositional/quantificational language with new logical constants which are  propositional connectives, quantifiers or predicates is logically true in the  pretheoretical sense iff it is true in all classical propositional/quantificational  models which (re)interpret its constants (other than its logical constants).
</blockquote>

<p>The problem with (T2) is that, no matter how one understands the notion of truth in a model that  appears in its formulation, and given a natural choice of logical constants, it is obviously false, and  it is pretty absurd to think that Tarski might have had something like this in mind. Elementary  considerations show that classical propositional or quantificational models are not appropriate for a  theory of the logical properties of the modal logical constants, such as &#8216;necessarily&#8217;.  A third thesis, that is perhaps true but again weak in a sense, and at any rate not Tarskian, is    </p>

<blockquote>
(T3) A sentence of a classical propositional/quantificational/modal language is  logically true in the pretheoretical sense iff it is true in all  propositional/quantificational/Kripke models which (re)interpret its constants  (other than its classical propositional/quantificational/modal logical constants).
</blockquote>

<p>(T3) talks about a very restricted set of (non-modal) logical constants, and it talks about a non-Tarskian notion of model.   Here is a fourth thesis:    </p>

<blockquote>
(T4) For every formal language <em>L</em> which possibly extends a classical  propositional/quantificational language with new propositional connectives,  quantifiers and predicates, there is a peculiar class <em>C</em> of models for the language  and a notion of truth in a model for L and C such that: a sentence <em>S</em> of <em>L</em> is logically true in the pretheoretical sense iff <em>S</em> is true in all models in <em>C</em>.
</blockquote>

<p>This cannot have been Tarski&#8217;s Thesis, for many reasons. For one thing, (T4) is too abstract. For  another, he clearly saw his thesis as open to refutation, while (T4) can be easily shown to be true.  (Nevertheless, the truth of (T4) indicates that in some sense the abstract model-theoretic approach is  especially suited to the task of characterizing relativized notions of logical truth.)  In my view, which can be historically supported, Tarski&#8217;s Thesis was this:   </p>

<blockquote>(T5) A sentence of a formal language which possibly extends a classical  propositional/quantificational language with new <em>extensional</em> logical constants which are propositional connectives, quantifiers and predicates is logically true in  the pretheoretical sense iff it is true in all classical propositional/quantificational models which (re)interpret its constants (other than its (extensional) logical constants)
</blockquote>

<p>(for a certain intuitive notion of extensionality that I characterize straightforwardly).  </p>

<p>Is (T5) true? It is certainly not obviously false. And to a good extent it seems to underlie the  practice of using classical models in the model theory of languages having as logical constants  generalized quantifiers, the predicate of identity, and other constants denoting notions invariant  under permutations of the quantificational domain. But I note that (T5) is false, under very reasonable assumptions which are familiar from standard logical practice. The first assumption is  that a monadic predicate &#8216;E&#8217; meaning &#8220;exists&#8221; is a logical constant. It may be noted that it is a logical constant both under &#8220;pragmatic&#8221; conceptions of logical constanthood and under Tarskian &#8220;permutationist&#8221; conceptions. (It denotes a notion invariant under permutations of actual domains.  This notion is also invariant under homomorphisms of actual domains, in the sense recently defined by Feferman.) The second assumption is that &#8216;E&#8217; is to be satisfied by an object at a world if that  object exists at that world. &#8216;E&#8217; can be given a simple satisfaction clause in the definition of  satisfaction in a classical quantificational model, very much like the clause for identity: a model <em>M</em> plus valuation <em>v</em> satisfies E(<em>x</em>) if <em>v</em>(<em>x</em>) exists. &#8216;E&#8217; thus understood is an extensional predicate.   </p>

<p>The third assumption is that a quantifier &#8216;(forall <em>x</em>)&#8217; with the following intuitive semantics is a  logical constant: the propositional contribution of &#8216;(forall <em>x</em>)&#8217; to a proposition expressed by a sentence  containing &#8216;(forall <em>x</em>)&#8217; gets specified when one simply specifies a quantificational domain, given purely  in extension, which constitutes the range of &#8216;(forall <em>x</em>)&#8217; specifies the set {you, me}, and that is enough.  </p>

<p>This notion of quantificational propositional content arguably underlies the classical  quantificational model conception of the quantifiers, and is reflected in one standard semantics for  quantified modal logic as follows: This notion of quantificational propositional content arguably underlies the classical  quantificational model conception of the quantifiers, and is reflected in one standard semantics for  quantified modal logic as follows: (forall <em>x</em>)<em>A</em> is said to be satisfied at a world <em>w</em> by a valuation <em>v</em> of the  variables with objects of a previously given domain <em>D</em> iff every valuation <em>u</em> of the variables with  objects of <em>D</em> which differs from <em>v</em> at most at &#8216;<em>x</em>&#8217; satisfies <em>A</em> at <em>w</em>.  </p>

<p>Given these assumptions, it is not difficult to exhibit sentences which are model-theoretic  logical truths in the sense of (T5), but are not logically true in a presumably relevant pretheoretical  sense. For example, the sentence  </p>

<blockquote>
(*) (forall <em>x</em>)E(<em>x</em>)
</blockquote>

<p>is a model-theoretic logical truth in the sense of (T5): no matter what classical quantificational  model we choose, (*) will be true in the model (classical models have actual domains). But (*) can  be used to express contingent propositions, such as the proposition that you and I exist. Provided  we take necessity as a mark of pretheoretical logical truth (as Tarski apparently did not care to do,  but most philosophers do), it follows that (*) is not logically true.  </p>
</div>
<div class="gray"><h3>Andrew Jorgensen (University of Leeds)</h3></div>
<div class="title"><p>Inferentialism and the meaning of non-denoting terms</p>
</div>
<div class="abstract"><p>The question is &#8220;Can we explain how non-denoting terms like &#8216;Father Christmas&#8217; are meaningful?&#8221; Prima facie, there is reason to hope a linguistic theory that does not treat reference or denotation as an explanatory primitive may provide such an explanation, since its explanatory concepts are available to be used. In this paper I sketch an inferentialist account of the meaning of some non-denoting terms.</p>

<p>I focus not on proper names but on a certain type of label that occurs within the philosophical practice I call the statement labelling convention (SLC). Writers in analytic philosophical tradition use the SLC to highlight significant propositions or sentences for discussion and comparison in their articles. The labels of the SLC standardly label sentences or propositions. I show how we can make sense of non-denoting labels in this practice. In particular, I show how the normative structure of the SLC generates inferential (and substitutional) proprieties of the sort required by Robert Brandom’s recent inferentialist theory (1994, and 2000). These inferential proprieties, and in particular the substitution inferential proprieties, give the meaning of the (non-denoting) labels at the heart of this paper. I conclude with some general remarks about how the lesson may be extended to &#8216;Father Christmas&#8217; and &#8216;Sherlock Holmes&#8217; and other non-denoting singular terms.</p>

<p>The approach developed in the paper has some merits worth mentioning. First, I show how the meaning of non-denoting terms stems from their part in a normative social practice just as the meaning of successfully denoting terms does. Hence I show how Brandom’s inferentialism provides a uniform treatment for both types of terms. This is relevant for the comparison of inferentialism with other accounts of the semantics of names. Second I head off possible squeamishness due to the apparently amorphous and versatile nature of normative social practices by beginning with a familiar (already recognised) practice that can be identified in advance of the theoretical work I expect it to do. Third, my work serves to relieve the dearth of examples in Brandom&#8217;s account of his programme (I try to actually derive the meaning of a particular expression (not a logical constant) from its place in a language game).</p>
</div>
<div class="gray"><h3>Nils K&uuml;bris (Universit&auml;t Leipzig)</h3></div>
<div class="title"><p>Negation, Relevance and the Theory of Meaning</p>
</div>
<div class="abstract"><p>The paper assumes a largely Dummettian framework for the justification of logical laws and a certain priority of proof over truth. The paper dissolves certain arguments by Dummett against classical logic, on the basis that negation is not amenable to proof-theoretic justification. However, classical logic cannot, given Dummett’s principles for self-justifying logical laws, be the whole of logic; in particular its implication connective is problematic. I argue that from a Dummettian position, somewhat modified in the light of the forgoing considerations, relevance logic can be viewed as employing only connectives governed by self-justifying rules of inference. But arguably, the same is true for intuitionistic logic. So surprisingly, the upshot is a certain pluralism, contrary to what one might expect from Dummett’s writings. Whether the laws governing negation are (quasi-)classical or (quasi)intuitionistic cannot be decided on the basis of a proof-theoretic justification of logical laws. Other meaning-theoretical resources need to be appealed to in order to settle this question. The paper concludes with some reflections on whether <em>ex contradictione quodlibet</em>, common to both, intuitionistic and classical logic, can be quite in accordance with Dummett&#8217;s arguments against holism. </p>

<p>Dummett&#8217;s proof-theoretic argument against classical logic hinges on the fact that we cannot formulate harmonious introduction and elimination rules for classical negation that satisfy Dummett&#8217;s constraints on self-justifying rules of inference. Adding classical negation to the positive fragment of a natural deduction formulation of logic does not result in a conservative extension thereof. Classical negation violates the principle of compositionality, i.e. that to understand a sentence I only need to understand its constituents, and the complexity condition, i.e. that the verification of a sentence proceeds only via less complex sentences: there are sentences A such that A can only be proven by showing that ~A leads to absurdity. Intuitionistic negation, on the contrary, is purportedly shown by Dummett to be capable of a proof theoretic justification. However, I shall give an that also intuitionistic negation cannot be so justified, as in order to justify a connective as a negation, in the introduction and elimination rules there already is implicitly assumed a notion of negation. The meaning of negation hence cannot be given by introduction and elimination rules for the symbol ~. The argument naturally leads to an adoption of a point by Peter Geach, that Fa and ~Fa are of the same complexity and can only be understood together. The complexity condition can then of course be trivially met. But meaning is compositional: ~Fa is composed out of Fa and ~, as negation makes a systematic contribution to the meanings of sentences in which it occurs. Understanding ~Fa and Fa is one thing, rejecting compositionality quite another thing. I shall argue that there still is a problem for the classical logician, as he cannot meet the compositionality constraint. This is due to theorems like Av(A>B), ((A>B)>A)>A and (A>B)v(B>A), which cannot be proved, in a system of natural deduction, without appeal to negation rules. If a logical law is true only in virtue of the logical constants occurring in it, then the conclusion must be that either v and > are not genuine logical constants or that we cannot construe the meanings of those theorems as composed out of the meanings of v and >. Now Geach&#8217;s point is one about predicates and the classical logician may certainly extend it to all kinds of sentences and may claim that the meanings of > and v also cannot be given by merely appealing to introduction and elimination rules for the constants, but in addition rules for ~(AvB) and ~(A>B) are needed. Once more, I shall argue that this is just to reject compositionality. The upshot of the discussion of classical logic is that at least its > connective is not an independent operation on sentences, contrary to what is suggested by rules governing it in the usual natural deduction systems, and that classical logic is best understood as talking only about ~ and &amp;. If implication tells us something that is independent of conjunction and negation, then > is not a proper implication connective. </p>

<p>This way relevance logic lies. Of course it is no surprise that my conclusion on classical logic follows from Dummett’s considerations. Classical logic, formulated using only ~ and &amp;, is a subsystem of intuitionistic logic. The question then is rather: is classical logic all of logic? Accepting Dummett&#8217;s criteria, the answer must be: no. The rules governing the implication connective of a relevance logic like <em>R</em> can easily be seen to be justified by Dummettian criteria and hence determine the meaning of &rarr; entirely. The problem for classical > iis one concerning the non-conservativeness of classical negation. Concerning <em>R</em>, the complexity and compositionality conditions can be met, given the conservativeness of negation over the positive fragment of <em>R</em> and the Geachean view on negation. <em>R</em> is, from this perspective, probably the most natural relevance logic, as it uses no restrictions on reiteration. Geach&#8217;s point does not in itself establish (quasi-)classical negation rules. I argue that both, the intuitionistic negation rules and the quasi-classical negation rules of <em>R</em> are reasonable from different perspectives. The result is that classical, intuitionist and relevant logic are all perfectly in order, from the proof-theoretic point of view, although reflecting on Dummett&#8217;s rejection of holism puts some doubt on <em>ex contradictione quodlibet</em> as a rule fundamentally connected to the meaning of negation. Whether there is a concept of negation which is primary and settles certain issues in the realism/antirealism debate cannot be established by appeal to proof-theoretical considerations only.</p>
</div>
<div class="gray"><h3>Greg Restall (University of Melbourne)</h3></div>
<div class="title"><p>Assertion and Denial, Commitment and Entitlement, and Incompatibility</p>
</div>
<div class="abstract"><p>In this short paper, I compare and contrast the kind of symmetricalist treatment of negation favoured in different ways by Huw Price (in &#8220;Why &#8216;Not&#8217;?&#8221;) and by me (in &#8220;Multiple Conclusions&#8221;) with Robert Brandom&#8217;s analysis of scorekeeping in terms of commitment, entitlement and incompatibility.</p>

<p>Both kinds of account provide a way to distinguish the inferential significance of &#8220;<em>A</em>&#8221; and &#8220;<em>A</em> is warranted&#8221; in terms of a subtler analysis of our practices: on the one hand, we assert as well as deny; on the other, by distingushing downstream commitments from upstream entitlements and the incompatibility definable in terms of these.  In this note I will examine the connections between these different approaches.</p>
</div>
<div class="gray"><h3>Gillian Russell (Princeton)</h3></div>
<div class="title"><p>Perjoratives</p>
</div>
<div class="abstract"><p>Sometimes the meaning of a pejorative expression (such as &#8216;nigger&#8217;, &#8216;homo&#8217; or &#8216;dork&#8217;) is taken to be fixed by some of the entailment relations that sentences containing it enter into.  For example, it might be thought that the meaning of the word &#8216;Boche&#8217; is fixed by its place in the argument forms:  </p>

<p><em>Boche-introduction</em>:  <em>x</em> is German.  Therefore, <em>x</em> is Boche.  </p>

<p><em>Boche-elimination</em>:  <em>x</em> is Boche.  Therefore, <em>x</em> is cruel.  </p>

<p>One objection to this kind of account is that such rules fail to determine a coherent extension for &#8216;Boche.&#8217;  (Consider Helmut, a kindly German.  Is he in the extension of &#8216;Boche&#8217;?  The first rule says &#8220;yes&#8221;, the second &#8220;no.&#8221;)  On some accounts this will mean that sentences containing the word fail to express propositions, and objectors suggest that our semantic theory would do better to make &#8216;Boche&#8217; a synonym of &#8216;German&#8217; and let pragmatics handle the rest.  In this paper I show how liberalising the notion of the extension of a predicate&#8212;to permit predicates to have non-exhaustive, and even non-exclusive extensions and anti-extensions&#8212;allows a response to this objection.</p>
</div>
<div class="gray"><h3>Inna Semetsky (Monash University)</h3></div>
<div class="title"><p>Abductive inference, meaning, and complexity: a dynamic structure</p>
</div>
<div class="abstract"><p>This paper draws from pragmatism, exemplified in the works of such philosophers as Peirce and Dewey. The word &#8220;structure&#8221; in the title asserts the primacy of relations over elements. Structure is a systemic property; the system&#8217;s elements being generated out of the relations that relate them. I use the word &#8220;dynamic&#8221; to emphasize the structure&#8217;s processual character in agreement with process metaphysics common to pragmatism. The word &#8220;complexity&#8221; purports to drive home the idea of the development, growth, and synthesis involved in the process, as well as being used in its mathematical sense (see below). The inferential process under discussion derives from Peirce’s semiotics, or the theory of signs; the signs broadly understood as both linguistic and extra-linguistic. Intentional content is an instance of structure. This paper will demonstrate that for the structure to be functional&#8212;or to have meaning, in a pragmatic sense&#8212;it must include a Peircean relation of Firstness, or abduction. Abduction, in the current philosophy of science discourse, is usually taken in one sense only, as an inference to the best explanation; this paper will posit abduction as open to interpretation in psychological and, quite possibly, naturalistic terms. In brief, abduction is a type of inference aiming at the creation of new meanings, rather than plainly representing the old ones.</p>

<p>First, I will introduce three basic ontological categories of Firstness, Secondness, and Thirdness, which Peirce indeed considered to be the &#8220;conceptions of complexity&#8221; (Peirce CP 1. 526). The cardinal nature of these categories assures that Thirdness must include Firstness in itself. This means that in terms of reasoning abduction is a mode of inference representing the necessary condition without which it will be impossible for the mind to function &#8220;properly&#8221;: deduction solely is insufficient. To illustrate this point I will offer a novel model of inference that incorporates Peircean categories. The mathematical formalism is borrowed from Gauss who left us an ingenious interpretation of a complex number. I will however expand on Gauss by suggesting that the modes of inference may be represented as vectors on a complex plane. Because of the model’s triadic character, in its totality is represents a relation between preconceptual ideas and conceptual cognitive representations. Integration of the former constitutes a dynamic production of meanings. In this respect, abductive inference acquires a naturalistic interpretation: it is not just in the head. Triadic process leads to establishing some stable structures in repeated experiences, that is, habits. The model as presented agrees also with John Dewey&#8217;s logic as a pragmatic theory of inquiry and his emphasis on precognitive (non-propositional) element in the meaning structure. </p>

<p>I will conclude this paper by asserting that my model overcomes the analytic paradox of new knowledge that haunted us since Plato first articulated it in the <em>Meno</em>.</p>
</div>
<div class="gray"><h3>N. J. J. Smith (Wellington)</h3></div>
<div class="title"><p>Semantic Regularity and the Liar Paradox</p>
</div>
<div class="abstract"><p>I present a solution to the Liar paradox which has the following features: (i) it is one hundred percent classical; (ii) it does not generate a strengthened Liar paradox or revenge problem; and (iii) the entrenched belief which the solution asks us to relinquish&#8212;and of course it must ask us to give up some such belief, for the Liar would not be a <em>paradox</em> if it could be solved without giving up anything&#8212;(a) is a belief in philosophy of language, not logic; (b) is a quite general belief about the operation of language, rather than a particular belief about a certain class of words&#8212;in particular it is not a belief specifically about truth; and (c) is also the key to solving a host of other deep philosophical problems. Jumping ahead, the belief is this: </p>

<blockquote>
<b>Semantic Regularity</b> 
There are reliable, principled relationships between our behaviour, mental states and physical environment on the one hand, and what we mean by our utterances on the other hand.
</blockquote>

<p>The relevance to the present Workshop is as follows. It is a consequence of my view that even if we introduce a linguistic item by stipulating that it should be governed (only) by certain inferential principles&#8212;for example, if we stipulate that the predicate &#8216;is true&#8217; should satisfy the T-schema, or should satisfy the pair of rules <em>P</em> / &#8216;P&#8217; is true, and &#8216;P&#8217; is true / <em>P</em>&#8212;the actual meaning of this linguistic item might be such that it does not always satisfy these principles. The meanings of our terms sometimes outrun anything we do to fix those meanings. </p>

<p>The paper proceeds as follows:</p>

<p>Section 1. I very briefly review/present the basics of the classical framework (as found in standard logic texts): the syntactic specification of a language as a set of wfs formed form certain symbols; the notion of an interpretation; and the recursive assignment of truth values to wfs on an interpretation. (Notation that will be used later: for any interpretation M, let TM be the set of wfs of the language which come out true on Mby the standard recursive definition.) I then discuss how to apply this picture to natural languages: I discuss how we manage to utter wfs (which are sequences, in the mathematical sense, of the symbol objects, and hence do not literally flow form our mouths or pens) by producing noises and making ink marks etc.; and I introduce the notion of the intended or correct interpretation of an utterance, with truth simpliciter of an utterance being truth relative to the correct interpretation. </p>

<p>Section 2.  I briefly present the Liar paradox. When we translate the Liar reasoning into the terms of the classical framework we have not a paradox, but a proof: a proof that there is no interpretation M on which the name <em>a</em> is assigned the wf ~<em>Ta</em> as its referent and the predicate <em>T</em> is assigned the set TM as its extension. This means that when I write &#8216;This sentence is not true&#8217;, and thereby utter a wf ~<em>Ta</em>, on the correct interpretation of my utterance either a does not refer to the wf I uttered, or <em>T</em> does not refer to the set TM. In plain language, either I do not refer to the sentence I utter, or I do not say of what I refer to that it is not true. </p>

<p>Sections 3 to 6. This, I say, is the correct solution to the Liar paradox. Recognising, however, that this will not be immediately convincing, I consider a series of objections to this view. Along the way, I draw several parallels between the liar paradox and the auto-infanticide paradox (which arises in connection with backwards time travel&#8212;a variant is known as the Grandfather paradox). Eventually the objections lead to Semantic Regularity: it turns out to be the core belief standing in the way of acceptance of the pure classical solution outlined above. </p>

<p>Section 7. My claim is not, however, that rejecting Semantic Regularity is one way of dealing with the Liar paradox; it is the much stronger claim that the Liar forces us to reject Semantic Regularity. The bane of all known solutions to the Liar is that they generate strengthened Liars or revenge problems. I argue that any approach to the Liar which generates a revenge problem either (a) ends up being incomplete or resorting to hand-waving, or (b) ends up asking us to accept something which amounts to a rejection of Semantic Regularity. I illustrate this claim with reference to Kripke’s theory of truth. Thus rejecting Semantic Regularity is not just one more strategy for solving the Liar; rather, the true lesson of the Liar is that Semantic Regularity fails. Either we can heed this lesson at the start (as I do), or we can postpone the lesson until it comes back in the guise of a strengthened Liar or revenge problem. </p>

<p>Sections 8 to 10. Having argued that we must abandon Semantic Regularity, I outline some of the independent payoffs of doing so. First, we undercut the key premise in semantic indeterminacy arguments such as those of Quine, Davidson, Putnam and Kripkenstein. Second, we solve the problem of empty names (there aren’t any). Third, we solve a recalcitrant problem in the literature on vagueness, which affects every theory which attempts to describe a semantics of vagueness in non-vague terms: the problem of false precision. (In a one-hour session I would discuss only the semantic indeterminacy arguments, and leave out the sections on empty names and vagueness.) </p>
</div>
<div class="gray"><h3>Bernard Weiss (University of Cape Town)</h3></div>
<div class="title"><p>Anti-realism and Truth, Meaning and Inference</p>
</div>
<div class="abstract"><p>The planned paper, somewhat ambitiously, tackles three, heavily inter-related, themes. My aim is to motivate and outline my preferred form for an anti-realist theory of meaning. </p>

<ol>
<li><p>The paradox of knowability is supposed to show that, short of accepting the radical implausibility that there is no truth which is unknown, the anti-realist cannot explain a notion of truth as knowability. I argue that the source of the problem presents us with a somewhat different lesson. The paradox assumes that we have a coherent systematic understanding of knowability. And this assumption is, I claim, thoroughly suspect (both in relation to knowability and in relation to other candidates for epistemic accounts of truth such as superassertibility).  I don&#8217;t, however, think that this is a grave situation for the anti-realist since: (a) it doesn&#8217;t frustrate her attempts to place epistemic constraints on truth; and (b) she can pursue her project by both eschewing a realist notion of truth and rejecting truth-conditional accounts of meaning (rather than opting for an anti-realist-truth-conditional account of meaning). Thus an anti-realist account of meaning will thus be one which fixes on appropriate conditions of use and delivers an account of truth in Tarskian fashion, i.e., under the presumption that meanings are given. The account of meaning will take rather different forms depending on whether the sentences under consideration are defeasible or not. Both sorts of account face a challenge in meeting Dummett&#8217;s requirement of molecularity.</p></li>
<li><p>In the case of defeasible sentences the worry is that no single-component theory of meaning will be able to account for the meanings of sentences. For here many sentences will share the putative meaning-determining aspect of use&#8212;for instance, assertion conditions&#8212;but will diverge in another aspect of use&#8212;in, say, defeating conditions. This sort of consideration, of course, figures prominently in Dummett&#8217;s discussion of the source and role of our notion of truth and prompts some to adopt a truth-conditional account of meaning. Now, if the choice for an anti-realist is between an unviable single component meaning theory and a truth-conditional account (which I&#8217;ve argued she should reject), then her position seems bleak indeed. Dummett himself seems to force this choice on the anti-realist since he argues that molecularity will require imposition of a constraint of harmony according to which the theory of meaning is forced to adopt the single component model. I elucidate what I take to be the motivation for the molecularity requirement and show that it can be made compatible with dual component meaning theories. So something like a Brandomian account of meaning can be reconciled with some of Dummett&#8217;s central motivations. The argument for the dual component conception precisely doesn&#8217;t apply in the case of indefeasible sentences: those of logic and mathematics.</p></li>
<li><p>The challenge faced by proof theoretic accounts of the meanings of the logical connectives is that they too contravene the molecularity requirement unless we distinguish between proofs and canonical proofs. I argue that the distinction won’t suffice to alleviate the difficulty unless it is supplemented with an account of the relation between proofs and canonical proofs and this requires a more nuanced understanding of the notions of applying and yielding as these figure in the account of the meanings of the logical connectives and in the account of the notion of proof (a proof being a construction which yields a canonical proof). The promise of such an account is that it can explain both a species of a priori knowledge and the usefulness of deduction; a sentence will be knowable a priori because of the character of its canonical proof-conditions (and thus true in virtue of its meaning) but may well be a truth which is informative to us because we can only recognise its truth by recognising that a certain construction yields an appropriate canonical proof. The notion of yielding which is implicated in the account of proof, unlike that implicated in the explanations of the meanings of the logical connectives, will be one which allows for the possibility of our coming thus to recognise a sentence as true which we are unable otherwise to recognise as true.</p></li>
</ol>
</div>


</div>

</body>